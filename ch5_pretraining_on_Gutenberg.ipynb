{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolina-p/NLP-with-Transformers/blob/main/ch5_pretraining_on_Gutenberg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBs6VC7xPoV1"
      },
      "source": [
        "# **Optimizing GPT-124M pretraining function on Hugging Face Streaming Datasets to run on one 40GB A100 GPU**\n",
        "GPTModel class comes from the book *LLMs from scratch by Sebastian Rashka*.\n",
        "\n",
        "\n",
        "**\"Bonus pretraining on Gutenberg\" solution:**\n",
        "https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg\n",
        "\n",
        "**Download:** \"*As of this writing, this will require approximately **50 GB** of disk space and take **about 10-15 hours**, but it may be more depending on how much Project Gutenberg grew since then.*\"\n",
        "\n",
        "**Training:** \"Warning: Note that training on 1 of the ~500 Mb text files in the gutenberg_preprocessed folder will take approximately **4 hours on a V100 GPU**. The folder contains 47 files and will take approximately **200 hours (more than 1 week) to complete**. You may want to run it on a smaller number of files.\"*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**The goal**: optimize\n",
        "- data loading,\n",
        "- training function, and\n",
        "- the GPT model\n",
        "\n",
        "to train on a \"big\" dataset with limited resources on Google Colab A100 runntime.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**The dataset**\n",
        "\n",
        "Original Project Gutenberg dataset: https://huggingface.co/datasets/manu/project_gutenberg\n",
        "\n",
        "- English split\n",
        "\n",
        "- 61.3K rows (books: book ID, text)  ||  38.026 unique rows\n",
        "\n",
        "This dataset contained duplicate books, excessive new lines, and blank spaces, as well as generic headers and footers. After cleaning and tokenizing the texts, the dataset was prepared for training and uploaded to the Hugging Face Hub.\n",
        "\n",
        "Clean and tokenized dataset: https://huggingface.co/datasets/nikolina-p/gutenberg_clean_tokenized_en_splits  \n",
        "\n",
        "- Total number of tokens after cleaning: 3_638_561_697\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GsDLg7wXA07j"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tiktoken \\\n",
        "-U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljurwmWFTT1"
      },
      "source": [
        "## **GPT MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU4jykP2lBkV"
      },
      "source": [
        "### **Multi-head self-attention mechanism**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hqqZKTdjfaUH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout) # preventing overfitting - only used in training\n",
        "\n",
        "        # non-trainable parameters part of the model's state, move and save/load with the model\n",
        "        self.register_buffer(\"mask\", torch.triu(\n",
        "            torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # b - batches\n",
        "\n",
        "        queries = self.W_query(x) # b x d_in x d_in\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # creating HEADs: step 1\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # creating HEADs: step 2\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        att_scores = queries @ keys.transpose(2,3) # dot product\n",
        "\n",
        "        att_scores = att_scores.masked_fill(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens],\n",
        "            -torch.inf\n",
        "        )\n",
        "\n",
        "        # scale and normalize\n",
        "        att_weights = torch.softmax(\n",
        "            att_scores / keys.shape[-1] ** 0.5,\n",
        "            dim=-1\n",
        "            )\n",
        "\n",
        "        att_weights = self.dropout(att_weights)\n",
        "\n",
        "        context_vec = att_weights @ values # # (b, num_tokens, num_heads, head_dim)\n",
        "\n",
        "        # reverse shaping\n",
        "        context_vec = context_vec.transpose(1, 2) # (b, num_heads, num_tokens, head_dim)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MDKtbPKghI_Y"
      },
      "outputs": [],
      "source": [
        "#@title Flash Attention\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class MultiHeadAttentionFlash(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = dropout # preventing overfitting - only used in training\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # b - batches\n",
        "\n",
        "        queries = self.W_query(x) # b x d_in x d_in\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        context_vec = F.scaled_dot_product_attention(queries, keys, values, dropout_p=0.1, is_causal=True)\n",
        "\n",
        "        # reverse shaping\n",
        "        context_vec = context_vec.transpose(1, 2) # (b, num_heads, num_tokens, head_dim)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST: compare the resulting tensors of flash and classic attention"
      ],
      "metadata": {
        "id": "_HWPE-Q0nBze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCLDum9o5daL",
        "outputId": "10246e31-52e8-43d4-ad77-a0a5440c1d2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2961, 0.5166, 0.2517, 0.6886, 0.0740, 0.8665]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "x = torch.rand(1, 1, 6)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpYmZDCb6FXT",
        "outputId": "cbd2b80d-7bf0-4035-f5e4-c587b56ade4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3871,  0.0798,  0.1245,  0.1996,  0.1424,  0.1624]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# create classic attn\n",
        "classic = MultiHeadAttention(d_in=6, d_out=6, context_length=1, dropout=0.1, num_heads=3)\n",
        "y = classic(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flash attn\n",
        "flash = MultiHeadAttentionFlash(d_in=6, d_out=6, dropout=0.1, num_heads=3)\n",
        "flash.load_state_dict(classic.state_dict(), strict=False)\n",
        "\n",
        "flash(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co4KTtZUkfbC",
        "outputId": "af9ef74e-c515-43c6-87d2-cd9e80cfc46e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3871,  0.0798,  0.1245,  0.1996,  0.1424,  0.1624]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRMZdgpgZpPX"
      },
      "source": [
        "Dokazala =>: Flash attention radi isto sto i moj model samo pre finalne normalizacije.\n",
        "\n",
        "Flash attention vrati kontekst vektor iscepkan na glave, pa se u svakoj sekvenci, glave moraju spojiti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGX9P5Izla0j"
      },
      "source": [
        "### **Feedforwar layer**\n",
        "**MLP - Multilayer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q4HHHZYbLaHz"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5 # helps avoid division by 0\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim)) # learnable\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # learnable\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "smWywjd7RIt0"
      },
      "outputs": [],
      "source": [
        "# activation function\n",
        "class GELU(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
        "                ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NEmW681mkBDd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWKVBHdbsIEW"
      },
      "source": [
        "### **Transformer Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P7zSayE3K5uQ"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg, flash=False):\n",
        "        super().__init__()\n",
        "        self.flash = flash\n",
        "\n",
        "        if self.flash:\n",
        "            self.att = MultiHeadAttentionFlash(\n",
        "            d_in=cfg[\"emb_dim\"], # dimension of input embeddings\n",
        "            d_out=cfg[\"emb_dim\"], # dimension of output embeddings\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"], # rate used for dropout\n",
        "            qkv_bias=cfg[\"qkv_bias\"]) # True/False - use bias in query, key and value weights matrices\n",
        "        else:\n",
        "            self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"], # dimension of input embeddings\n",
        "            d_out=cfg[\"emb_dim\"], # dimension of output embeddings\n",
        "            context_length=cfg[\"context_length\"], # number of tokens in one sequence\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"], # rate used for dropout\n",
        "            qkv_bias=cfg[\"qkv_bias\"]) # True/False - use bias in query, key and value weights matrices\n",
        "\n",
        "        self.ff = FeedForward(cfg)\n",
        "\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"]) # dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-Head Self-Attention Layer\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        # Feed-Forward Layer\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DljxVF_tsLkA"
      },
      "source": [
        "### **GPT MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-d0TzbEukBah"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "\n",
        "class GPTModel(nn.Module, PyTorchModelHubMixin):\n",
        "\n",
        "    def __init__(self, cfg, flash=False, tied=False):\n",
        "        super().__init__()\n",
        "        self.flash = flash\n",
        "        self.tied = tied\n",
        "\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg, self.flash) for _ in range(cfg[\"n_layers\"])]\n",
        "            )\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "        if self.tied:\n",
        "            self.out_head.weight = self.tok_emb.weight\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(\n",
        "            torch.arange(seq_len, device=in_idx.device)\n",
        "            )\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "\n",
        "        x = self.trf_blocks(x)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u5SgsGwlTwk"
      },
      "source": [
        "## **GPT configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K0p1uqxjl92r"
      },
      "outputs": [],
      "source": [
        "# the model coded in this notebook has 163M parameters (no weight tying)\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257, # Vocabulary size\n",
        "    \"context_length\": 512, # Context length\n",
        "    \"emb_dim\": 768, # Embedding dimension\n",
        "    \"n_heads\": 12, # Number of attention heads\n",
        "    \"n_layers\": 12, # Number of layers\n",
        "    \"drop_rate\": 0.1, # Dropout rate\n",
        "    \"qkv_bias\": False # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQCBVrPWEU2x"
      },
      "source": [
        "## **StreamingDataset**\n",
        "\n",
        "https://huggingface.co/docs/datasets/v4.0.0/en/about_mapstyle_vs_iterable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hrrxoLfY01x3"
      },
      "outputs": [],
      "source": [
        "#@title streaming from tokenized dataset\n",
        "import random, torch\n",
        "from datasets import IterableDataset\n",
        "\n",
        "class StreamingDataset(IterableDataset):\n",
        "    \"\"\"An iterable dataset that generates input-target sequence pairs,\n",
        "    and shuffles sequences at the book level.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, context_size):\n",
        "        self.dataset = iter(dataset)\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            try:\n",
        "                book = next(self.dataset)\n",
        "                book_token_ids = book[\"tokenized\"]\n",
        "\n",
        "                if len(book_token_ids) < self.context_size:\n",
        "                    print(f\"Book {book['id']} too short - not enough tokens.\")\n",
        "                    continue\n",
        "\n",
        "                # loop trough shuffled start_indices of input_chunk(s) and create pairs\n",
        "                for i in self.shuffle_indices(len(book_token_ids)):\n",
        "                    input_chunk = book_token_ids[i:i + self.context_size]\n",
        "                    target_chunk = book_token_ids[i + 1:i + self.context_size + 1]\n",
        "                    yield book[\"id\"], torch.tensor(input_chunk), torch.tensor(target_chunk)\n",
        "            except StopIteration:\n",
        "                print(\"StreamingDataset: no more data.\")\n",
        "                break\n",
        "\n",
        "    def shuffle_indices(self, book_num_tokens):\n",
        "        \"\"\"shuffles START INDICES of input chunks\"\"\"\n",
        "        start_indices = list(range(0, book_num_tokens - self.context_size, self.context_size))\n",
        "        random.shuffle(start_indices)\n",
        "        return start_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX8mLeSrFTT-"
      },
      "source": [
        "## **Training function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oUjg5fPWFTT-"
      },
      "outputs": [],
      "source": [
        "# calculates the loss per one batch\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2moaF8ngoMEN"
      },
      "source": [
        "####**Save checkpoint and training results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G5QlGDebFTUB"
      },
      "outputs": [],
      "source": [
        "# save the model and optimizer state\n",
        "def save_checkpoint(model, optimizer, num_books, scaler=None):\n",
        "    print(f\"Saving checkpoint...{num_books}\")\n",
        "    torch.save({\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scaler_state_dict\": scaler.state_dict() if scaler else None\n",
        "        },\n",
        "        f\"checkpoint_cycle_{num_books}.pth\"\n",
        "        )\n",
        "\n",
        "#load the model and optimizer\n",
        "def load_checkpoint(file_name, model_config):\n",
        "    checkpoint = torch.load(file_name, map_location=device)\n",
        "    model = GPTModel(model_config)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eHFBwpb7FTUA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# save training results in json file\n",
        "def save_training_info(train_losses, valid_losses, books_seen, total_batches, times=None):\n",
        "    data = {\n",
        "        \"total_batches\": total_batches,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"books_seen\": books_seen,\n",
        "        \"time per cycle\": times if times else []\n",
        "    }\n",
        "\n",
        "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    with open(f\"results_{date_str}_books_{len(books_seen)}.json\", \"w\", encoding=\"UTF-8\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "def plot_loss_convergence(train_losses, val_losses):\n",
        "    cycles = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(cycles, train_losses, label='Training Loss', marker='o')\n",
        "    plt.plot(cycles, val_losses, label='Validation Loss', marker='s')\n",
        "    plt.xlabel('Cycle')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpK8nXWqZwlQ"
      },
      "source": [
        "### **1. baseline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vr3UHE02Hpet"
      },
      "outputs": [],
      "source": [
        "#@title Train model - no optimization\n",
        "from collections import Counter\n",
        "\n",
        "def train_model_simple(model, dataloader_train, dataloader_valid, optimizer, device, num_epoch, training_batches, val_ratio):\n",
        "    # track losses per cycle and tokens seen\n",
        "    train_losses, valid_losses = [], []\n",
        "    total_batches = 0\n",
        "    tokens_seen_train, tokens_seen_valid = 0, 0\n",
        "    books_seen = Counter()\n",
        "\n",
        "    # Main training loop\n",
        "    try:\n",
        "        for epoch in range(num_epoch):\n",
        "            loss_train, train_batch_count = 0, 0 # accumulated loss in a cycle; number of batches in a cycle\n",
        "            loss_valid, valid_batch_count = 0, 0\n",
        "\n",
        "            model.train()\n",
        "            start = time.time()\n",
        "\n",
        "            # LOAD FROM TRAIN SPLIT\n",
        "            for book_id_batch, input_batch, target_batch in dataloader_train:\n",
        "                books_seen.update(book_id_batch)\n",
        "\n",
        "                # train\n",
        "                model.train()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                loss.backward() # Calculate loss gradients\n",
        "                optimizer.step() # Update model weights using loss gradients\n",
        "\n",
        "                loss_train += loss\n",
        "                tokens_seen_train += input_batch.numel()\n",
        "                train_batch_count += 1\n",
        "                total_batches += 1\n",
        "\n",
        "                if train_batch_count % training_batches == 0:\n",
        "                    # LOAD FROM VALIDATION SPLIT\n",
        "                    for book_id_batch, input_batch, target_batch in dataloader_valid:\n",
        "                        books_seen.update(book_id_batch)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            model.eval()\n",
        "                            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                            loss_valid += loss\n",
        "                            tokens_seen_valid += input_batch.numel()\n",
        "                            valid_batch_count += 1\n",
        "                            total_batches += 1\n",
        "\n",
        "                            if valid_batch_count % (training_batches * val_ratio) == 0:\n",
        "                                # end train/valid cycle and print results\n",
        "                                torch.cuda.synchronize()\n",
        "                                end = time.time()\n",
        "\n",
        "                                train_losses.append((loss_train / train_batch_count).item())\n",
        "                                loss_train = 0\n",
        "                                train_batch_count = 0\n",
        "\n",
        "                                valid_losses.append((loss_valid / valid_batch_count).item())\n",
        "                                loss_valid = 0\n",
        "                                valid_batch_count = 0\n",
        "\n",
        "                                tok_sec = (input_batch.numel()*training_batches*(1+val_ratio))/(end-start)\n",
        "                                print(f\"\\nbatches:{total_batches} | loss: {train_losses[-1]:.3f}/{valid_losses[-1]:.3f}\" \\\n",
        "                                    f\"| tok-seen: {tokens_seen_train+tokens_seen_valid:,}\" \\\n",
        "                                    f\"| time: {(end-start):,.2f}\" \\\n",
        "                                    f\"| tok/sec: {tok_sec:,.2f} | epoch time: {3638561697/(3600*tok_sec):,.2f} hrs\")\n",
        "\n",
        "                                start = time.time()\n",
        "                                break\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSaving model after KeyboardInterrupt\")\n",
        "\n",
        "    print(f\"\\nTokens seen (train/val/total) {tokens_seen_train:,} / {tokens_seen_valid:,} / {(tokens_seen_train + tokens_seen_valid):,}\")\n",
        "    print(f\"Total books seen {len(books_seen):,}\")\n",
        "    save_checkpoint(model, optimizer, len(books_seen))\n",
        "    save_training_info(train_losses, valid_losses, books_seen, total_batches)\n",
        "\n",
        "    return train_losses, valid_losses, total_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoNRNyStCaS7"
      },
      "source": [
        "### **2. mixed precision**\n",
        "\n",
        "“Automatic mixed precision training” means training with `torch.autocast` and `torch.amp.GradScaler` together.\n",
        "\n",
        "**Autocasting** automatically chooses the precision for operations to improve performance while maintaining accuracy.\n",
        "\n",
        "`torch.amp.GradScaler` helps perform the steps of gradient scaling conveniently. Gradient scaling improves convergence for networks with float16 (by default on CUDA and XPU) gradients by minimizing gradient underflow (flush to zero).\n",
        "\n",
        "If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.\n",
        "\n",
        "To prevent underflow, **“gradient scaling” multiplies the network’s loss(es) by a scale factor** and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.\n",
        "\n",
        "Each parameter’s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.\n",
        "\n",
        "receipt: https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cuBLAS** is NVIDIA’s GPU-accelerated implementation of the BLAS (Basic Linear Algebra Subprograms) library:\n",
        "- Vector and matrix multiplication (e.g., GEMM: General Matrix Multiply)\n",
        "- Matrix-vector products\n",
        "- Vector dot products and norms\n",
        "\n",
        "**cuDNN** is NVIDIA’s GPU-accelerated library specifically for deep learning primitives.\n",
        "- Convolution operations (for CNNs)\n",
        "- Pooling (max, average)\n",
        "- Normalization (batch norm, layer norm)\n",
        "- Activation functions (ReLU, tanh, sigmoid)\n",
        "- RNNs (LSTM, GRU)\n",
        "- Tensor layout transformations"
      ],
      "metadata": {
        "id": "fUprmceQP5Ak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lar2xetqDhmq",
        "outputId": "5c6cc9b8-27d8-4df5-ec1f-16887ebe0740"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "hi  libcublas-12-5                         12.5.3.2-1                              amd64        CUBLAS native runtime libraries\n",
            "hi  libcublas-dev-12-5                     12.5.3.2-1                              amd64        CUBLAS native dev links, headers\n",
            "hi  libcudnn9-cuda-12                      9.2.1.18-1                              amd64        cuDNN runtime libraries for CUDA 12.5\n",
            "ii  libcudnn9-dev-cuda-12                  9.2.1.18-1                              amd64        cuDNN development headers and symlinks for CUDA 12.5\n",
            "Torch: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!dpkg -l | grep libcublas\n",
        "!dpkg -l | grep cudnn\n",
        "print(f\"Torch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_TgJp57HI9Jj"
      },
      "outputs": [],
      "source": [
        "#@title Train model - mixed precision\n",
        "from collections import Counter\n",
        "\n",
        "def train_model_mixed_precision(model, train_loader, valid_loader, optimizer, device, num_epoch, training_batches, val_ratio):\n",
        "    train_losses, valid_losses = [], []  # track losses per cycle\n",
        "    tokens_seen_train, tokens_seen_valid = 0, 0\n",
        "    books_seen = Counter()\n",
        "    total_batches = 0\n",
        "\n",
        "    # Main training loop\n",
        "    try:\n",
        "        scaler = torch.GradScaler('cuda')\n",
        "        start = time.time()\n",
        "        end = 0\n",
        "\n",
        "        for epoch in range(num_epoch):\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            loss_train, loss_valid = 0, 0 # accumulated loss in a cycle\n",
        "            train_batch_count, valid_batch_count = 0, 0 # number of batches in a cycle\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            for book_id_batch, input_batch, target_batch in train_loader:\n",
        "                books_seen.update(book_id_batch)\n",
        "\n",
        "                # train\n",
        "                model.train()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Runs the forward pass with autocasting.\n",
        "                with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "                    loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "                # Scales loss. Calls backward() on scaled loss to create scaled gradients.\n",
        "                # Backward passes under autocast are not recommended.\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # `scaler.step()` first unscales the gradients of the optimizer's assigned params.\n",
        "                # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
        "                # otherwise, optimizer.step() is skipped.\n",
        "                scaler.step(optimizer)\n",
        "\n",
        "                # Updates the scale for next iteration.\n",
        "                scaler.update()\n",
        "\n",
        "                loss_train += loss\n",
        "                tokens_seen_train += input_batch.numel()\n",
        "                train_batch_count += 1\n",
        "                total_batches += 1\n",
        "\n",
        "                if train_batch_count % training_batches == 0:\n",
        "                    # LOAD FROM VALIDATION SPLIT\n",
        "                    for book_id_batch, input_batch, target_batch in valid_loader:\n",
        "                        books_seen.update(book_id_batch)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            model.eval()\n",
        "                            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "                                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                            loss_valid += loss\n",
        "                            tokens_seen_valid += input_batch.numel()\n",
        "                            valid_batch_count += 1\n",
        "                            total_batches += 1\n",
        "\n",
        "                        if valid_batch_count % (int(training_batches * val_ratio)) == 0:\n",
        "                            # print results of the previous training/validation cycle, and clean tracking vars\n",
        "                            torch.cuda.synchronize()\n",
        "                            end = time.time()\n",
        "\n",
        "                            train_losses.append((loss_train / train_batch_count).item())\n",
        "                            loss_train = 0\n",
        "                            train_batch_count = 0\n",
        "\n",
        "                            valid_losses.append((loss_valid / valid_batch_count).item())\n",
        "                            loss_valid = 0\n",
        "                            valid_batch_count = 0\n",
        "\n",
        "                            tok_sec = (input_batch.numel() * int(training_batches * (1+val_ratio))) / (end-start)\n",
        "                            print(f\"\\nbatches:{total_batches} | loss: {train_losses[-1]:.3f}/{valid_losses[-1]:.3f}\" \\\n",
        "                                    f\"| tok-seen: {tokens_seen_train+tokens_seen_valid:,}\" \\\n",
        "                                    f\"| time: {(end-start):,.2f}\" \\\n",
        "                                    f\"| tok/sec: {tok_sec:,.2f} | epoch: {3638561697/(3600*tok_sec):,.2f} hrs\")\n",
        "                            start = time.time()\n",
        "                            break\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSaving model after KeyboardInterrupt\")\n",
        "\n",
        "\n",
        "    print(f\"\\nTokens seen: {tokens_seen_train:,} / {tokens_seen_valid:,} / {(tokens_seen_train + tokens_seen_valid):,}\")\n",
        "    print(f\"Total books seen {len(books_seen):,}\")\n",
        "    save_checkpoint(model, optimizer, len(books_seen), scaler)\n",
        "    save_training_info(train_losses, valid_losses, books_seen, total_batches)\n",
        "\n",
        "    return train_losses, valid_losses, total_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypxBBYIeFTUA"
      },
      "source": [
        "#**MAIN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwVJ5O8jITx7"
      },
      "source": [
        "## **Streaming from separate train and validation splits**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "26f1eb65dc1144559e0e17b47da908cb",
            "7d361bdd329a46d2b197e50e21af7f69",
            "72f115ad4650403d994aedf87af1d2bf",
            "b75220daa13e453da21d174b27de814b",
            "84edc2075d034a628134b5662676541b",
            "c2fd1ff4c7824fe5873248720b7a380e",
            "62bb269f2e7b49a094cab454243f7fdb",
            "1975c0febf6d4fd2ac304504ebc9e43b",
            "212d4b90892f47299afaa6ed8529b154",
            "19c630c8f4184738a13d9fc7a31c0c73",
            "a5c343e7cc36482096cce5c792e6dc67",
            "b075ca4c17084a06899f9ed733e65b6a",
            "e03f9283d6984689b2e7bc421a3e4be1",
            "dc5e6c81885740d9812b53e0c6c98dce",
            "4c2297651086490ca23a47bbb70c6b82",
            "ab3b0390139f48738df2bd671daa6002",
            "f9704af76e8d44c49bbc068e970b7efd",
            "badfcf8321144f5681d3ec2abed9e6ff",
            "b284f4412f3241e5a56aa16c67f9e5e1",
            "ea5e3ef523314ed0baa60c955042c708",
            "15beb05441a8482f8bbcaed873441dbd",
            "f4ad408ff0544f7eba627e8459bf9183",
            "4ed491ec17654b89b51ede88734b8898",
            "bf2f9fb2c389460b84f5e0c9d818649d",
            "0d861e5e68fa44fc98bae1701f0b1e35",
            "167f74a95fe84b2dbd689523e41a57fb",
            "ac0743355ddd49b28fb4aa4b3e8f54fd",
            "14aef7c6bb434d7a9cb7042801f70a63",
            "0cc7629303784c50996e9d20a0297f32",
            "177a1fab30a542509f96c8597be9b3b4",
            "42ef27980cb54d6fb65ef975d1d77b8a",
            "c71b6cb4b56a437f981a2806c219fd6a",
            "48c14cc914c642b08f17b982f3a9ef37"
          ]
        },
        "id": "etH2yHgF01ij",
        "outputId": "865dfd0e-c3c7-4c4e-f480-9f52a5e470e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26f1eb65dc1144559e0e17b47da908cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b075ca4c17084a06899f9ed733e65b6a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ed491ec17654b89b51ede88734b8898"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gutenberg_train = load_dataset(\"nikolina-p/gutenberg_clean_tokenized_en_splits\", split=\"train\", streaming=True, columns=[\"id\", \"tokenized\"])\n",
        "gutenberg_valid = load_dataset(\"nikolina-p/gutenberg_clean_tokenized_en_splits\", split=\"validation\", streaming=True, columns=[\"id\", \"tokenized\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E39b3nojfZ8",
        "outputId": "3c9c2df5-6361-49ad-b41e-98507521b211"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IterableDataset({\n",
              "    features: ['id', 'tokenized'],\n",
              "    num_shards: 33\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuTJ2xTQjSA7",
        "scrolled": true,
        "outputId": "652b5604-5075-4d6e-8e71-fe720f654505"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "\n",
            "batches:220 | loss: 32.894/13.998| tok-seen: 7,208,960| time: 110.54| tok/sec: 65,214.56 | epoch: 15.50 hrs\n",
            "\n",
            "batches:440 | loss: 12.910/8.938| tok-seen: 14,417,920| time: 49.25| tok/sec: 146,384.86 | epoch: 6.90 hrs\n",
            "\n",
            "batches:660 | loss: 9.607/7.965| tok-seen: 21,626,880| time: 48.88| tok/sec: 147,477.63 | epoch: 6.85 hrs\n",
            "\n",
            "batches:880 | loss: 8.569/7.328| tok-seen: 28,835,840| time: 49.12| tok/sec: 146,766.49 | epoch: 6.89 hrs\n",
            "\n",
            "Saving model after KeyboardInterrupt\n",
            "\n",
            "Tokens seen: 30,277,632 / 2,621,440 / 32,899,072\n",
            "Total books seen 350\n",
            "Saving checkpoint...350\n",
            "Training completed in 4.78 minutes.\n"
          ]
        }
      ],
      "source": [
        "import pdb\n",
        "import time\n",
        "import tiktoken\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#GPT_CONFIG_124M[\"vocab_size\"] = ((GPT_CONFIG_124M[\"vocab_size\"] + 127) // 128) * 128\n",
        "model = GPTModel(GPT_CONFIG_124M, flash=False, tied=False)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "#model = torch.compile(model)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "ds_train = StreamingDataset(gutenberg_train, context_size=GPT_CONFIG_124M['context_length'])\n",
        "ds_valid = StreamingDataset(gutenberg_valid, context_size=GPT_CONFIG_124M['context_length'])\n",
        "\n",
        "\n",
        "loader_train = DataLoader(ds_train,\n",
        "                          batch_size=28,\n",
        "                          drop_last=True,\n",
        "                          num_workers=2,\n",
        "                          pin_memory=True\n",
        "                          )\n",
        "\n",
        "loader_valid = DataLoader(ds_valid,\n",
        "                          batch_size=28,\n",
        "                          drop_last=True,\n",
        "                          num_workers=2,\n",
        "                          pin_memory=True\n",
        "                          )\n",
        "fun = 'mix'\n",
        "start_time = time.time()\n",
        "match fun:\n",
        "    case 'baseline': # autocast-only\n",
        "        #torch.backends.cuda.matmul.allow_tf32 = True # enable Tensor Cores\n",
        "        #torch.set_float32_matmul_precision('high')\n",
        "        train_losses, valid_losses, total_batches = train_model_simple(model,\n",
        "                                                                       loader_train,\n",
        "                                                                       loader_valid,\n",
        "                                                                       optimizer,\n",
        "                                                                       device,\n",
        "                                                                       num_epoch=2,\n",
        "                                                                       training_batches=10,\n",
        "                                                                       val_ratio=0.1\n",
        "                                                                       )\n",
        "    case 'mix': # autocast and grad scaler\n",
        "        #torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.set_float32_matmul_precision('high') # enable Tensor Cores\n",
        "        train_losses, valid_losses, total_batches = train_model_mixed_precision(model,\n",
        "                                                                                loader_train,\n",
        "                                                                                loader_valid,\n",
        "                                                                                optimizer,\n",
        "                                                                                device,\n",
        "                                                                                num_epoch=2,\n",
        "                                                                                training_batches=200,\n",
        "                                                                                val_ratio=0.1\n",
        "                                                                                )\n",
        "    case _:\n",
        "        print(\"Unknown choice\")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uLrwXrYInVZs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pljurwmWFTT1",
        "DljxVF_tsLkA",
        "2moaF8ngoMEN",
        "1xkDRVrDiJwV",
        "NCEjlxA7bE6M",
        "HkiYHAhqlzFS",
        "OkaLFx29bKi8",
        "1TfuNgQnADmt"
      ],
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOQ5EJ2DYazVVGTZWEyEv4B",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "26f1eb65dc1144559e0e17b47da908cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7d361bdd329a46d2b197e50e21af7f69",
              "IPY_MODEL_72f115ad4650403d994aedf87af1d2bf",
              "IPY_MODEL_b75220daa13e453da21d174b27de814b"
            ],
            "layout": "IPY_MODEL_84edc2075d034a628134b5662676541b"
          }
        },
        "7d361bdd329a46d2b197e50e21af7f69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2fd1ff4c7824fe5873248720b7a380e",
            "placeholder": "​",
            "style": "IPY_MODEL_62bb269f2e7b49a094cab454243f7fdb",
            "value": "README.md: "
          }
        },
        "72f115ad4650403d994aedf87af1d2bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1975c0febf6d4fd2ac304504ebc9e43b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_212d4b90892f47299afaa6ed8529b154",
            "value": 1
          }
        },
        "b75220daa13e453da21d174b27de814b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c630c8f4184738a13d9fc7a31c0c73",
            "placeholder": "​",
            "style": "IPY_MODEL_a5c343e7cc36482096cce5c792e6dc67",
            "value": " 3.28k/? [00:00&lt;00:00, 260kB/s]"
          }
        },
        "84edc2075d034a628134b5662676541b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2fd1ff4c7824fe5873248720b7a380e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62bb269f2e7b49a094cab454243f7fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1975c0febf6d4fd2ac304504ebc9e43b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "212d4b90892f47299afaa6ed8529b154": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19c630c8f4184738a13d9fc7a31c0c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c343e7cc36482096cce5c792e6dc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b075ca4c17084a06899f9ed733e65b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e03f9283d6984689b2e7bc421a3e4be1",
              "IPY_MODEL_dc5e6c81885740d9812b53e0c6c98dce",
              "IPY_MODEL_4c2297651086490ca23a47bbb70c6b82"
            ],
            "layout": "IPY_MODEL_ab3b0390139f48738df2bd671daa6002"
          }
        },
        "e03f9283d6984689b2e7bc421a3e4be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9704af76e8d44c49bbc068e970b7efd",
            "placeholder": "​",
            "style": "IPY_MODEL_badfcf8321144f5681d3ec2abed9e6ff",
            "value": "Resolving data files: 100%"
          }
        },
        "dc5e6c81885740d9812b53e0c6c98dce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b284f4412f3241e5a56aa16c67f9e5e1",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea5e3ef523314ed0baa60c955042c708",
            "value": 33
          }
        },
        "4c2297651086490ca23a47bbb70c6b82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15beb05441a8482f8bbcaed873441dbd",
            "placeholder": "​",
            "style": "IPY_MODEL_f4ad408ff0544f7eba627e8459bf9183",
            "value": " 33/33 [00:00&lt;00:00, 72.73it/s]"
          }
        },
        "ab3b0390139f48738df2bd671daa6002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9704af76e8d44c49bbc068e970b7efd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "badfcf8321144f5681d3ec2abed9e6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b284f4412f3241e5a56aa16c67f9e5e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea5e3ef523314ed0baa60c955042c708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15beb05441a8482f8bbcaed873441dbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4ad408ff0544f7eba627e8459bf9183": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ed491ec17654b89b51ede88734b8898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bf2f9fb2c389460b84f5e0c9d818649d",
              "IPY_MODEL_0d861e5e68fa44fc98bae1701f0b1e35",
              "IPY_MODEL_167f74a95fe84b2dbd689523e41a57fb"
            ],
            "layout": "IPY_MODEL_ac0743355ddd49b28fb4aa4b3e8f54fd"
          }
        },
        "bf2f9fb2c389460b84f5e0c9d818649d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14aef7c6bb434d7a9cb7042801f70a63",
            "placeholder": "​",
            "style": "IPY_MODEL_0cc7629303784c50996e9d20a0297f32",
            "value": "Resolving data files: 100%"
          }
        },
        "0d861e5e68fa44fc98bae1701f0b1e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_177a1fab30a542509f96c8597be9b3b4",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42ef27980cb54d6fb65ef975d1d77b8a",
            "value": 33
          }
        },
        "167f74a95fe84b2dbd689523e41a57fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c71b6cb4b56a437f981a2806c219fd6a",
            "placeholder": "​",
            "style": "IPY_MODEL_48c14cc914c642b08f17b982f3a9ef37",
            "value": " 33/33 [00:00&lt;00:00, 3909.94it/s]"
          }
        },
        "ac0743355ddd49b28fb4aa4b3e8f54fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14aef7c6bb434d7a9cb7042801f70a63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cc7629303784c50996e9d20a0297f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "177a1fab30a542509f96c8597be9b3b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42ef27980cb54d6fb65ef975d1d77b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c71b6cb4b56a437f981a2806c219fd6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48c14cc914c642b08f17b982f3a9ef37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}