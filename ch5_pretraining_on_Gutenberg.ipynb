{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikolina-p/NLP-with-Transformers/blob/main/ch5_pretraining_on_Gutenberg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBs6VC7xPoV1"
      },
      "source": [
        "# **Optimizing GPT-124M pretraining function on Hugging Face Streaming Datasets to run on one 40GB A100 GPU**\n",
        "GPTModel class comes from the book *LLMs from scratch by Sebastian Rashka*.\n",
        "\n",
        "\n",
        "**\"Bonus pretraining on Gutenberg\" solution:**\n",
        "https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg\n",
        "\n",
        "**Download:** \"*As of this writing, this will require approximately **50 GB** of disk space and take **about 10-15 hours**, but it may be more depending on how much Project Gutenberg grew since then.*\"\n",
        "\n",
        "**Training:** \"Warning: Note that training on 1 of the ~500 Mb text files in the gutenberg_preprocessed folder will take approximately **4 hours on a V100 GPU**. The folder contains 47 files and will take approximately **200 hours (more than 1 week) to complete**. You may want to run it on a smaller number of files.\"*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**The goal**: optimize\n",
        "- data loading,\n",
        "- training function, and\n",
        "- the GPT model\n",
        "\n",
        "to train on a \"big\" dataset with limited resources on Google Colab A100 runntime.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**The dataset**\n",
        "\n",
        "Original Project Gutenberg dataset: https://huggingface.co/datasets/manu/project_gutenberg\n",
        "\n",
        "- English split\n",
        "\n",
        "- 61.3K rows (books: book ID, text)  ||  38.026 unique rows\n",
        "\n",
        "This dataset contained duplicate books, excessive new lines, and blank spaces, as well as generic headers and footers. After cleaning and tokenizing the texts, the dataset was prepared for training and uploaded to the Hugging Face Hub.\n",
        "\n",
        "Clean and tokenized dataset: https://huggingface.co/datasets/nikolina-p/gutenberg_clean_tokenized_en_splits  \n",
        "\n",
        "- Total number of tokens after cleaning: 3_638_561_697\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GsDLg7wXA07j"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tiktoken \\\n",
        "-U datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljurwmWFTT1"
      },
      "source": [
        "## **GPT MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DU4jykP2lBkV"
      },
      "source": [
        "### **Multi-head self-attention mechanism**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hqqZKTdjfaUH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = nn.Dropout(dropout) # preventing overfitting - only used in training\n",
        "\n",
        "        # non-trainable parameters part of the model's state, move and save/load with the model\n",
        "        self.register_buffer(\"mask\", torch.triu(\n",
        "            torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # b - batches\n",
        "\n",
        "        queries = self.W_query(x) # b x d_in x d_in\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # creating HEADs: step 1\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # creating HEADs: step 2\n",
        "        queries = queries.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        att_scores = queries @ keys.transpose(2,3) # dot product\n",
        "\n",
        "        att_scores = att_scores.masked_fill(\n",
        "            self.mask.bool()[:num_tokens, :num_tokens],\n",
        "            -torch.inf\n",
        "        )\n",
        "\n",
        "        # scale and normalize\n",
        "        att_weights = torch.softmax(\n",
        "            att_scores / keys.shape[-1] ** 0.5,\n",
        "            dim=-1\n",
        "            )\n",
        "\n",
        "        att_weights = self.dropout(att_weights)\n",
        "\n",
        "        context_vec = att_weights @ values # # (b, num_tokens, num_heads, head_dim)\n",
        "\n",
        "        # reverse shaping\n",
        "        context_vec = context_vec.transpose(1, 2) # (b, num_heads, num_tokens, head_dim)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MDKtbPKghI_Y"
      },
      "outputs": [],
      "source": [
        "#@title Flash Attention\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class MultiHeadAttentionFlash(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "        self.out_proj = nn.Linear(d_out, d_out)\n",
        "        self.dropout = dropout # preventing overfitting - only used in training\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # b - batches\n",
        "\n",
        "        queries = self.W_query(x) # b x d_in x d_in\n",
        "        keys = self.W_key(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        context_vec = F.scaled_dot_product_attention(queries, keys, values, dropout_p=0.1, is_causal=True)\n",
        "\n",
        "        # reverse shaping\n",
        "        context_vec = context_vec.transpose(1, 2) # (b, num_heads, num_tokens, head_dim)\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "\n",
        "        context_vec = self.out_proj(context_vec)\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### TEST: compare the resulting tensors of flash and classic attention"
      ],
      "metadata": {
        "id": "_HWPE-Q0nBze"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCLDum9o5daL",
        "outputId": "2887c345-c3a5-4faf-dbb5-9fbdd28bfce8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2961, 0.5166, 0.2517, 0.6886, 0.0740, 0.8665]]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "torch.set_printoptions(sci_mode=False)\n",
        "\n",
        "x = torch.rand(1, 1, 6)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpYmZDCb6FXT",
        "outputId": "e6e1aed7-2df3-4527-c7ca-080c4e327794"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3871,  0.0798,  0.1245,  0.1996,  0.1424,  0.1624]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# create classic attn\n",
        "classic = MultiHeadAttention(d_in=6, d_out=6, context_length=1, dropout=0.1, num_heads=3)\n",
        "y = classic(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flash attn\n",
        "flash = MultiHeadAttentionFlash(d_in=6, d_out=6, dropout=0.1, num_heads=3)\n",
        "flash.load_state_dict(classic.state_dict(), strict=False)\n",
        "\n",
        "flash(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co4KTtZUkfbC",
        "outputId": "1f6adb23-bf4e-4978-ff64-37ea389d2847"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.3871,  0.0798,  0.1245,  0.1996,  0.1424,  0.1624]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGX9P5Izla0j"
      },
      "source": [
        "### **Feedforwar layer**\n",
        "**MLP - Multilayer Perceptron**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "q4HHHZYbLaHz"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5 # helps avoid division by 0\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim)) # learnable\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # learnable\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "smWywjd7RIt0"
      },
      "outputs": [],
      "source": [
        "# activation function\n",
        "class GELU(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))\n",
        "                ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NEmW681mkBDd"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWKVBHdbsIEW"
      },
      "source": [
        "### **Transformer Block**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "P7zSayE3K5uQ"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg, flash=False):\n",
        "        super().__init__()\n",
        "        self.flash = flash\n",
        "\n",
        "        AttClass = MultiHeadAttentionFlash if self.flash else MultiHeadAttention\n",
        "\n",
        "        self.att = AttClass(\n",
        "            d_in=cfg[\"emb_dim\"], # dimension of input embeddings\n",
        "            d_out=cfg[\"emb_dim\"], # dimension of output embeddings\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"], # rate used for dropout\n",
        "            qkv_bias=cfg[\"qkv_bias\"], # True/False - use bias in query, key and value weights matrices\n",
        "            **({\"context_length\": cfg[\"context_length\"]} if not self.flash else {})\n",
        "        )\n",
        "\n",
        "        self.ff = FeedForward(cfg)\n",
        "\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"]) # dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Multi-Head Self-Attention Layer\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        # Feed-Forward Layer\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DljxVF_tsLkA"
      },
      "source": [
        "### **GPT MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-d0TzbEukBah"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import PyTorchModelHubMixin\n",
        "\n",
        "class GPTModel(nn.Module, PyTorchModelHubMixin):\n",
        "\n",
        "    def __init__(self, cfg, flash=False, tied=False):\n",
        "        super().__init__()\n",
        "        self.flash = flash\n",
        "        self.tied = tied\n",
        "\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg, self.flash) for _ in range(cfg[\"n_layers\"])]\n",
        "            )\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
        "\n",
        "        if self.tied:\n",
        "            self.tok_emb.weight = self.out_head.weight\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(\n",
        "            torch.arange(seq_len, device=in_idx.device)\n",
        "            )\n",
        "        x = tok_embeds + pos_embeds\n",
        "        x = self.drop_emb(x)\n",
        "\n",
        "        x = self.trf_blocks(x)\n",
        "\n",
        "        x = self.final_norm(x)\n",
        "\n",
        "        logits = self.out_head(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u5SgsGwlTwk"
      },
      "source": [
        "## **GPT configuration**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K0p1uqxjl92r"
      },
      "outputs": [],
      "source": [
        "# the model coded in this notebook has 163M parameters (no weight tying)\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257, # Vocabulary size\n",
        "    \"context_length\": 512, # Context length\n",
        "    \"emb_dim\": 768, # Embedding dimension\n",
        "    \"n_heads\": 12, # Number of attention heads\n",
        "    \"n_layers\": 12, # Number of layers\n",
        "    \"drop_rate\": 0.1, # Dropout rate\n",
        "    \"qkv_bias\": False # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQCBVrPWEU2x"
      },
      "source": [
        "## **StreamingDataset**\n",
        "\n",
        "https://huggingface.co/docs/datasets/v4.0.0/en/about_mapstyle_vs_iterable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hrrxoLfY01x3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title streaming from tokenized dataset\n",
        "import random, torch\n",
        "from datasets import IterableDataset\n",
        "\n",
        "class StreamingDataset(IterableDataset):\n",
        "    \"\"\"An iterable dataset that generates input-target sequence pairs,\n",
        "    and shuffles sequences at the book level.\"\"\"\n",
        "\n",
        "    def __init__(self, dataset, context_size):\n",
        "        self.dataset = iter(dataset)\n",
        "        self.context_size = context_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            try:\n",
        "                book = next(self.dataset)\n",
        "                book_token_ids = book[\"tokenized\"]\n",
        "\n",
        "                if len(book_token_ids) < self.context_size:\n",
        "                    print(f\"Book {book['id']} too short - not enough tokens.\")\n",
        "                    continue\n",
        "\n",
        "                # loop trough shuffled start_indices of input_chunk(s) and create pairs\n",
        "                for i in self.shuffle_indices(len(book_token_ids)):\n",
        "                    input_chunk = book_token_ids[i:i + self.context_size]\n",
        "                    target_chunk = book_token_ids[i + 1:i + self.context_size + 1]\n",
        "                    yield book[\"id\"], torch.tensor(input_chunk), torch.tensor(target_chunk)\n",
        "            except StopIteration:\n",
        "                print(\"StreamingDataset: no more data.\")\n",
        "                break\n",
        "\n",
        "    def shuffle_indices(self, book_num_tokens):\n",
        "        \"\"\"shuffles START INDICES of input chunks\"\"\"\n",
        "        start_indices = list(range(0, book_num_tokens - self.context_size, self.context_size))\n",
        "        random.shuffle(start_indices)\n",
        "        return start_indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX8mLeSrFTT-"
      },
      "source": [
        "## **Training function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oUjg5fPWFTT-"
      },
      "outputs": [],
      "source": [
        "# calculates the loss per one batch\n",
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2moaF8ngoMEN"
      },
      "source": [
        "####**Save checkpoint and training results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G5QlGDebFTUB"
      },
      "outputs": [],
      "source": [
        "# save the model and optimizer state\n",
        "def save_checkpoint(model, optimizer, num_books, scaler=None):\n",
        "    print(f\"Saving checkpoint...{num_books}\")\n",
        "    torch.save({\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"scaler_state_dict\": scaler.state_dict() if scaler else None\n",
        "        },\n",
        "        f\"checkpoint_cycle_{num_books}.pth\"\n",
        "        )\n",
        "\n",
        "#load the model and optimizer\n",
        "def load_checkpoint(file_name, model_config):\n",
        "    checkpoint = torch.load(file_name, map_location=device)\n",
        "    model = GPTModel(model_config)\n",
        "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eHFBwpb7FTUA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# save training results in json file\n",
        "def save_training_info(train_losses, valid_losses, books_seen, total_batches, times=None):\n",
        "    data = {\n",
        "        \"total_batches\": total_batches,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"books_seen\": books_seen,\n",
        "        \"time per cycle\": times if times else []\n",
        "    }\n",
        "\n",
        "    date_str = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "    with open(f\"results_{date_str}_books_{len(books_seen)}.json\", \"w\", encoding=\"UTF-8\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "\n",
        "def plot_loss_convergence(train_losses, val_losses):\n",
        "    cycles = range(1, len(train_losses) + 1)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(cycles, train_losses, label='Training Loss', marker='o')\n",
        "    plt.plot(cycles, val_losses, label='Validation Loss', marker='s')\n",
        "    plt.xlabel('Cycle')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Convergence')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpK8nXWqZwlQ"
      },
      "source": [
        "### **1. baseline**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vr3UHE02Hpet"
      },
      "outputs": [],
      "source": [
        "#@title Train model - no optimization\n",
        "from collections import Counter\n",
        "\n",
        "def train_model_simple(model, dataloader_train, dataloader_valid, optimizer, device, num_epoch, training_batches, val_ratio):\n",
        "    # track losses per cycle and tokens seen\n",
        "    train_losses, valid_losses = [], []\n",
        "    total_batches = 0\n",
        "    tokens_seen_train, tokens_seen_valid = 0, 0\n",
        "    books_seen = Counter()\n",
        "\n",
        "    # Main training loop\n",
        "    try:\n",
        "        for epoch in range(num_epoch):\n",
        "            loss_train, train_batch_count = 0, 0 # accumulated loss in a cycle; number of batches in a cycle\n",
        "            loss_valid, valid_batch_count = 0, 0\n",
        "\n",
        "            model.train()\n",
        "            start = time.time()\n",
        "\n",
        "            # LOAD FROM TRAIN SPLIT\n",
        "            for book_id_batch, input_batch, target_batch in dataloader_train:\n",
        "                books_seen.update(book_id_batch)\n",
        "\n",
        "                # train\n",
        "                model.train()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                loss.backward() # Calculate loss gradients\n",
        "                optimizer.step() # Update model weights using loss gradients\n",
        "\n",
        "                loss_train += loss\n",
        "                tokens_seen_train += input_batch.numel()\n",
        "                train_batch_count += 1\n",
        "                total_batches += 1\n",
        "\n",
        "                if train_batch_count % training_batches == 0:\n",
        "                    # LOAD FROM VALIDATION SPLIT\n",
        "                    for book_id_batch, input_batch, target_batch in dataloader_valid:\n",
        "                        books_seen.update(book_id_batch)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            model.eval()\n",
        "                            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                            loss_valid += loss\n",
        "                            tokens_seen_valid += input_batch.numel()\n",
        "                            valid_batch_count += 1\n",
        "                            total_batches += 1\n",
        "\n",
        "                            if valid_batch_count % (training_batches * val_ratio) == 0:\n",
        "                                # end train/valid cycle and print results\n",
        "                                torch.cuda.synchronize()\n",
        "                                end = time.time()\n",
        "\n",
        "                                train_losses.append((loss_train / train_batch_count).item())\n",
        "                                loss_train = 0\n",
        "                                train_batch_count = 0\n",
        "\n",
        "                                valid_losses.append((loss_valid / valid_batch_count).item())\n",
        "                                loss_valid = 0\n",
        "                                valid_batch_count = 0\n",
        "\n",
        "                                tok_sec = (input_batch.numel()*training_batches*(1+val_ratio))/(end-start)\n",
        "                                print(f\"\\nbatches:{total_batches} | loss: {train_losses[-1]:.3f}/{valid_losses[-1]:.3f}\" \\\n",
        "                                    f\"| tok-seen: {tokens_seen_train+tokens_seen_valid:,}\" \\\n",
        "                                    f\"| time: {(end-start):,.2f}\" \\\n",
        "                                    f\"| tok/sec: {tok_sec:,.2f} | epoch time: {3638561697/(3600*tok_sec):,.2f} hrs\")\n",
        "\n",
        "                                start = time.time()\n",
        "                                break\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSaving model after KeyboardInterrupt\")\n",
        "\n",
        "    print(f\"\\nTokens seen (train/val/total) {tokens_seen_train:,} / {tokens_seen_valid:,} / {(tokens_seen_train + tokens_seen_valid):,}\")\n",
        "    print(f\"Total books seen {len(books_seen):,}\")\n",
        "    save_checkpoint(model, optimizer, len(books_seen))\n",
        "    save_training_info(train_losses, valid_losses, books_seen, total_batches)\n",
        "\n",
        "    return train_losses, valid_losses, total_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoNRNyStCaS7"
      },
      "source": [
        "### **2. mixed precision**\n",
        "\n",
        "“Automatic mixed precision training” means training with `torch.autocast` and `torch.amp.GradScaler` together.\n",
        "\n",
        "**Autocasting** automatically chooses the precision for operations to improve performance while maintaining accuracy.\n",
        "\n",
        "`torch.amp.GradScaler` helps perform the steps of gradient scaling conveniently. Gradient scaling improves convergence for networks with float16 (by default on CUDA and XPU) gradients by minimizing gradient underflow (flush to zero).\n",
        "\n",
        "If the forward pass for a particular op has float16 inputs, the backward pass for that op will produce float16 gradients. Gradient values with small magnitudes may not be representable in float16. These values will flush to zero (“underflow”), so the update for the corresponding parameters will be lost.\n",
        "\n",
        "To prevent underflow, **“gradient scaling” multiplies the network’s loss(es) by a scale factor** and invokes a backward pass on the scaled loss(es). Gradients flowing backward through the network are then scaled by the same factor. In other words, gradient values have a larger magnitude, so they don’t flush to zero.\n",
        "\n",
        "Each parameter’s gradient (.grad attribute) should be unscaled before the optimizer updates the parameters, so the scale factor does not interfere with the learning rate.\n",
        "\n",
        "receipt: https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**cuBLAS** is NVIDIA’s GPU-accelerated implementation of the BLAS (Basic Linear Algebra Subprograms) library:\n",
        "- Vector and matrix multiplication (e.g., GEMM: General Matrix Multiply)\n",
        "- Matrix-vector products\n",
        "- Vector dot products and norms\n",
        "\n",
        "**cuDNN** is NVIDIA’s GPU-accelerated library specifically for deep learning primitives.\n",
        "- Convolution operations (for CNNs)\n",
        "- Pooling (max, average)\n",
        "- Normalization (batch norm, layer norm)\n",
        "- Activation functions (ReLU, tanh, sigmoid)\n",
        "- RNNs (LSTM, GRU)\n",
        "- Tensor layout transformations"
      ],
      "metadata": {
        "id": "fUprmceQP5Ak"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lar2xetqDhmq",
        "outputId": "08ac6ede-3458-4d02-89d7-0478c02e6907"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "hi  libcublas-12-5                         12.5.3.2-1                              amd64        CUBLAS native runtime libraries\n",
            "hi  libcublas-dev-12-5                     12.5.3.2-1                              amd64        CUBLAS native dev links, headers\n",
            "hi  libcudnn9-cuda-12                      9.2.1.18-1                              amd64        cuDNN runtime libraries for CUDA 12.5\n",
            "ii  libcudnn9-dev-cuda-12                  9.2.1.18-1                              amd64        cuDNN development headers and symlinks for CUDA 12.5\n",
            "Torch: 2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version\n",
        "!dpkg -l | grep libcublas\n",
        "!dpkg -l | grep cudnn\n",
        "print(f\"Torch: {torch.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "_TgJp57HI9Jj"
      },
      "outputs": [],
      "source": [
        "#@title Train model - mixed precision\n",
        "from collections import Counter\n",
        "\n",
        "def train_model_mixed_precision(model, train_loader, valid_loader, optimizer, device, num_epoch, training_batches, val_ratio):\n",
        "    train_losses, valid_losses = [], []  # track losses per cycle\n",
        "    tokens_seen_train, tokens_seen_valid = 0, 0\n",
        "    books_seen = Counter()\n",
        "    total_batches = 0\n",
        "\n",
        "    # Main training loop\n",
        "    try:\n",
        "        scaler = torch.GradScaler('cuda')\n",
        "        start = time.time()\n",
        "        end = 0\n",
        "\n",
        "        for epoch in range(num_epoch):\n",
        "            print(f\"Epoch: {epoch}\")\n",
        "            loss_train, loss_valid = 0, 0 # accumulated loss in a cycle\n",
        "            train_batch_count, valid_batch_count = 0, 0 # number of batches in a cycle\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            for book_id_batch, input_batch, target_batch in train_loader:\n",
        "                books_seen.update(book_id_batch)\n",
        "\n",
        "                # train\n",
        "                model.train()\n",
        "                optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                # Runs the forward pass with autocasting.\n",
        "                with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "                    loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                loss_train += loss\n",
        "                tokens_seen_train += input_batch.numel()\n",
        "                train_batch_count += 1\n",
        "                total_batches += 1\n",
        "\n",
        "                if train_batch_count % training_batches == 0:\n",
        "                    # LOAD FROM VALIDATION SPLIT\n",
        "                    for book_id_batch, input_batch, target_batch in valid_loader:\n",
        "                        books_seen.update(book_id_batch)\n",
        "\n",
        "                        with torch.no_grad():\n",
        "                            model.eval()\n",
        "                            with torch.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "                                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                            loss_valid += loss\n",
        "                            tokens_seen_valid += input_batch.numel()\n",
        "                            valid_batch_count += 1\n",
        "                            total_batches += 1\n",
        "\n",
        "                        if valid_batch_count % (int(training_batches * val_ratio)) == 0:\n",
        "                            # print results of the previous training/validation cycle, and clean tracking vars\n",
        "                            torch.cuda.synchronize()\n",
        "                            end = time.time()\n",
        "\n",
        "                            train_losses.append((loss_train / train_batch_count).item())\n",
        "                            loss_train = 0\n",
        "                            train_batch_count = 0\n",
        "\n",
        "                            valid_losses.append((loss_valid / valid_batch_count).item())\n",
        "                            loss_valid = 0\n",
        "                            valid_batch_count = 0\n",
        "\n",
        "                            tok_sec = (input_batch.numel() * int(training_batches * (1+val_ratio))) / (end-start)\n",
        "                            print(f\"\\nbatches:{total_batches} | loss: {train_losses[-1]:.3f}/{valid_losses[-1]:.3f}\" \\\n",
        "                                    f\"| tok-seen: {tokens_seen_train+tokens_seen_valid:,}\" \\\n",
        "                                    f\"| time: {(end-start):,.2f}\" \\\n",
        "                                    f\"| tok/sec: {tok_sec:,.2f} | epoch: {3638561697/(3600*tok_sec):,.2f} hrs\")\n",
        "                            start = time.time()\n",
        "                            break\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nSaving model after KeyboardInterrupt\")\n",
        "\n",
        "\n",
        "    print(f\"\\nTokens seen: {tokens_seen_train:,} / {tokens_seen_valid:,} / {(tokens_seen_train + tokens_seen_valid):,}\")\n",
        "    print(f\"Total books seen {len(books_seen):,}\")\n",
        "    save_checkpoint(model, optimizer, len(books_seen), scaler)\n",
        "    save_training_info(train_losses, valid_losses, books_seen, total_batches)\n",
        "\n",
        "    return train_losses, valid_losses, total_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypxBBYIeFTUA"
      },
      "source": [
        "#**MAIN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwVJ5O8jITx7"
      },
      "source": [
        "## **Streaming from separate train and validation splits**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237,
          "referenced_widgets": [
            "aaafa1cc8e7e4fc19864a1ab19a4772a",
            "8bfc5084e08444c485786ffaee1d5c19",
            "50804b8c316442409183bc330f1bd4f7",
            "41395cdf38cf4d209f55c21adfcb99e6",
            "81f34221d2b24796a225145455e2e7d0",
            "76a79cd62ae64c248a7638a1223947fc",
            "a16103ba172b4149b7a8c0f523ef41a6",
            "e66f027413fc421da1e60fece25b7eb4",
            "7f9b203c03bb4397baacae1069a76d79",
            "25158c21696c4d8c8c6723f85409325a",
            "54753dd17ccc4dfba045f4320863fb9a",
            "a603af7ea9504f66952f861afbf45e78",
            "5f94be8cd1b642c7992e1e4dbc7fa4ff",
            "b87330a2a6e941fba0d902d358c6e190",
            "6d2ca2d21f0942449a93f20e4b484ed9",
            "ae5129e80aec4a7b9e6e230b9c0f20d8",
            "fb232d3924c04b5a934d99d6d5c0177e",
            "d8923918c8fd49da9632ab0763e95903",
            "db0b751c0c194b7b9bf2f3320c0b610a",
            "5c95bd1a6513402b96635c696df966ea",
            "6639c89653974c5c958ae78bda43942d",
            "78494f9e47ec4f34870c75081bfc01d9",
            "793a0dfb2b6845b58126a194d3fd10f0",
            "9127ba42c7c84742ad5f011cd0b5e5dd",
            "785cc0c723b04184a83f110e2e3bde56",
            "e18f6ea4a724455d8f5792b659b27c7c",
            "79c3f8a07e8d49e38aca0249304f2048",
            "55e4207e86aa42c9878659c01792c0e4",
            "6e6dd8396d134c6c9f9894b59d62b77c",
            "270841d734fb478fa4f3dc3efba8c4d4",
            "c56fd224ac5a4c1eb4653f06cacc33c6",
            "af25c4d0165a4410b3f52b526a893b84",
            "4501ad5fadd14d7cb1caf10e51304b85"
          ]
        },
        "id": "etH2yHgF01ij",
        "outputId": "cf0e50ef-a810-4cdd-a1bb-e5eb24099073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaafa1cc8e7e4fc19864a1ab19a4772a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a603af7ea9504f66952f861afbf45e78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "793a0dfb2b6845b58126a194d3fd10f0"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "gutenberg_train = load_dataset(\"nikolina-p/gutenberg_clean_tokenized_en_splits\", split=\"train\", streaming=True, columns=[\"id\", \"tokenized\"])\n",
        "gutenberg_valid = load_dataset(\"nikolina-p/gutenberg_clean_tokenized_en_splits\", split=\"validation\", streaming=True, columns=[\"id\", \"tokenized\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E39b3nojfZ8",
        "outputId": "7674b4a4-4852-4b8e-fa0b-9e732d228c2e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "IterableDataset({\n",
              "    features: ['id', 'tokenized'],\n",
              "    num_shards: 33\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_2E3FDc0oVPR",
        "outputId": "4b6b2bd1-7362-42c6-da42-155b6b9df1f9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 26 02:49:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:00:05.0 Off |                    0 |\n",
            "| N/A   31C    P0             54W /  400W |       0MiB /  81920MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuTJ2xTQjSA7",
        "scrolled": true,
        "outputId": "2e45f5e7-ce49-43cd-f505-e14e6680fbdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "\n",
            "batches:220 | loss: 7.448/7.287| tok-seen: 14,417,920| time: 144.17| tok/sec: 100,006.82 | epoch: 10.11 hrs\n",
            "\n",
            "batches:440 | loss: 7.300/7.215| tok-seen: 28,835,840| time: 84.42| tok/sec: 170,783.58 | epoch: 5.92 hrs\n",
            "\n",
            "batches:660 | loss: 7.267/7.259| tok-seen: 43,253,760| time: 83.62| tok/sec: 172,420.09 | epoch: 5.86 hrs\n",
            "\n",
            "Saving model after KeyboardInterrupt\n",
            "\n",
            "Tokens seen: 39,452,672 / 3,932,160 / 43,384,832\n",
            "Total books seen 447\n",
            "Saving checkpoint...447\n",
            "Training completed in 5.25 minutes.\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "import inspect\n",
        "\n",
        "#GPT_CONFIG_124M[\"vocab_size\"] = 55296\n",
        "GPT_CONFIG_124M[\"vocab_size\"] = ((GPT_CONFIG_124M[\"vocab_size\"] + 127) // 128) * 128\n",
        "model = GPTModel(GPT_CONFIG_124M, flash=True, tied=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model = torch.compile(model)\n",
        "\n",
        "fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "use_fused = fused_available and device.type == 'cuda'\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1, betas=(0.9, 0.95), fused=use_fused)\n",
        "\n",
        "ds_train = StreamingDataset(gutenberg_train, context_size=GPT_CONFIG_124M['context_length'])\n",
        "ds_valid = StreamingDataset(gutenberg_valid, context_size=GPT_CONFIG_124M['context_length'])\n",
        "\n",
        "loader_train = DataLoader(ds_train,\n",
        "                          batch_size=128,\n",
        "                          drop_last=True,\n",
        "                          num_workers=2,\n",
        "                          pin_memory=True\n",
        "                          )\n",
        "\n",
        "loader_valid = DataLoader(ds_valid,\n",
        "                          batch_size=128,\n",
        "                          drop_last=True,\n",
        "                          num_workers=2,\n",
        "                          pin_memory=True\n",
        "                          )\n",
        "fun = 'mix'\n",
        "start_time = time.time()\n",
        "match fun:\n",
        "    case 'baseline': # autocast-only\n",
        "        #torch.backends.cuda.matmul.allow_tf32 = True # enable Tensor Cores\n",
        "        torch.set_float32_matmul_precision('high')\n",
        "        train_losses, valid_losses, total_batches = train_model_simple(model,\n",
        "                                                                       loader_train,\n",
        "                                                                       loader_valid,\n",
        "                                                                       optimizer,\n",
        "                                                                       device,\n",
        "                                                                       num_epoch=2,\n",
        "                                                                       training_batches=10,\n",
        "                                                                       val_ratio=0.1\n",
        "                                                                       )\n",
        "    case 'mix': # autocast and grad scaler\n",
        "        #torch.backends.cuda.matmul.allow_tf32 = True\n",
        "        torch.set_float32_matmul_precision('high') # enable Tensor Cores\n",
        "        train_losses, valid_losses, total_batches = train_model_mixed_precision(model,\n",
        "                                                                                loader_train,\n",
        "                                                                                loader_valid,\n",
        "                                                                                optimizer,\n",
        "                                                                                device,\n",
        "                                                                                num_epoch=2,\n",
        "                                                                                training_batches=200,\n",
        "                                                                                val_ratio=0.1\n",
        "                                                                                )\n",
        "    case _:\n",
        "        print(\"Unknown choice\")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ..."
      ],
      "metadata": {
        "id": "lV4E5f6ARh2b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_2OIsqkT1M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TEST: Weights tying and loss\n",
        "Conclusion: Using the embedding layer's default initialization (-5 to +5) for shared weights between `tok_emb` and `out_head`, causes very large logits and unstable loss when tying weights. Linear layers initialize with a narrower distribution (~±0.02), which produces logits near ±3 and an initial loss close to ln(vocab_size) ≈ 10.8.\n",
        "\n",
        "By initializing tok_emb from out_head instead of the reverse, we ensure stable\n",
        "starting conditions for training and prevent exploding loss.\n",
        "`class GPTModel`\n",
        "\n",
        "`def __init__(self, cfg, flash=False, tied=False):`\n",
        "\n",
        "    if self.tied:\n",
        "            self.tok_emb.weight = self.out_head.weight # not reverse!`\n"
      ],
      "metadata": {
        "id": "iW77sGJi9QDJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### dummy data"
      ],
      "metadata": {
        "id": "a8ym19bLBWCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.randint(0, 50257, (16, 513), device='cuda')\n",
        "x = data[:, :-1]\n",
        "target = data[:, 1:]"
      ],
      "metadata": {
        "id": "_jR5XaxONOWN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OordFG4ROhFU",
        "outputId": "1f0ee077-73cd-4f9e-8b9b-be7827080454"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWghQanzOoy4",
        "outputId": "857b5d60-ca32-47a5-8abf-775b20b8a897"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZoPDwzdOu6W",
        "outputId": "6410e7ec-dbdf-442d-eb81-c7255bd8d9af"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18099, 29438, 35212, 41222,  9061, 26163, 44525,   893, 33492, 30653],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target[0,:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1-ElwHoO1bk",
        "outputId": "0ff51eb1-d1a1-4a6a-85ff-5a41ad7b10b5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([29438, 35212, 41222,  9061, 26163, 44525,   893, 33492, 30653, 48698],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model using embedding weights for tying"
      ],
      "metadata": {
        "id": "INKvl3tvBbG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = GPTModel(GPT_CONFIG_124M, flash=True, tied=False)\n",
        "model_1.out_head.weight = model_1.tok_emb.weight"
      ],
      "metadata": {
        "id": "m0r5Oc7kAYo_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_1.out_head.weight.data_ptr() == model_1.tok_emb.weight.data_ptr())\n",
        "model_1.to('cuda')\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wjtij4PeLii_",
        "outputId": "aa910320-71ef-4051-f947-481d983af122"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tok emb size: {model_1.tok_emb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxlT4PWWMFB8",
        "outputId": "baafb0cd-7e3f-4585-eb50-b65431f93f6b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tok emb size: Embedding(50257, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = model_1(x)"
      ],
      "metadata": {
        "id": "wkK2yPk0NQHi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29FGEAdONUes",
        "outputId": "04355121-b871-482e-874b-88d31104bd17"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(y.flatten(0, 1), target.flatten())"
      ],
      "metadata": {
        "id": "MC7ZBeqeN1K3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HUGE INITIAL LOSS!!!\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkPWHPDSR9V_",
        "outputId": "da43d54b-23ff-4714-f9d9-c4015288995d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(460.2455, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logits min:\", y.min().item(), \"max:\", y.max().item(), \"mean:\", y.mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qt7bZJ6iR97Z",
        "outputId": "9c03b114-4275-4082-be71-0ad1e6321c11"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits min: -159.7432403564453 max: 557.7025756835938 mean: 0.004521739669144154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tied weight min:\", model_1.out_head.weight.min().item())\n",
        "print(\"Tied weight max:\", model_1.out_head.weight.max().item())\n",
        "print(\"Embedding weight min:\", model_1.tok_emb.weight.min().item())\n",
        "print(\"Embedding weight max:\", model_1.tok_emb.weight.max().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O7sH7XPTe7p",
        "outputId": "a0d58ba8-1797-491c-814e-2deeb485e4ab"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tied weight min: -5.335026741027832\n",
            "Tied weight max: 5.475025177001953\n",
            "Embedding weight min: -5.335026741027832\n",
            "Embedding weight max: 5.475025177001953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### model using Linear layer weigths for tying"
      ],
      "metadata": {
        "id": "92DFGsDzBtDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = GPTModel(GPT_CONFIG_124M, flash=True, tied=False)\n",
        "model_2.tok_emb.weight = model_2.out_head.weight"
      ],
      "metadata": {
        "id": "XYwcHBwkUsgk"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_2.out_head.weight.data_ptr() == model_2.tok_emb.weight.data_ptr())\n",
        "model_2.to('cuda')\n",
        "print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f100b60-d0bd-4dae-9233-b1bc6310f89a",
        "id": "w4pgbSpiCS5d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Tok emb size: {model_2.tok_emb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845b962c-1037-4712-e8d7-e26cfafe2a73",
        "id": "GRHF-XZACS5e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tok emb size: Embedding(50257, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = model_2(x)"
      ],
      "metadata": {
        "id": "H3PnykUECS5e"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55c72882-42e8-4ddb-efbc-d6bf8ec19eba",
        "id": "jK3wXe_lCS5e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 512, 50257])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = torch.nn.functional.cross_entropy(y.flatten(0, 1), target.flatten())"
      ],
      "metadata": {
        "id": "TQaIN-nNCS5f"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Expected loss for random initialized parameters -> -ln(1/50257) = 10.8249 ✅\n",
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e062bc8b-1cbf-46cf-beb8-a3c80872e480",
        "id": "3S0ACRRwCS5f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(10.9947, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Logits min:\", y.min().item(), \"max:\", y.max().item(), \"mean:\", y.mean().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e7f2fb5-8fae-46bf-9dc0-9f82ea04e333",
        "id": "oY_atb68CS5f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits min: -3.2705678939819336 max: 3.3515560626983643 mean: 0.00011122244177386165\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tied weight min:\", model_2.out_head.weight.min().item())\n",
        "print(\"Tied weight max:\", model_2.out_head.weight.max().item())\n",
        "print(\"Embedding weight min:\", model_2.tok_emb.weight.min().item())\n",
        "print(\"Embedding weight max:\", model_2.tok_emb.weight.max().item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1072eddb-fc82-41f2-b237-599e4eedf696",
        "id": "6TrSoJzUCS5f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tied weight min: -0.03608439117670059\n",
            "Tied weight max: 0.036084387451410294\n",
            "Embedding weight min: -0.03608439117670059\n",
            "Embedding weight max: 0.036084387451410294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VFOEYwc1Euhw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2moaF8ngoMEN"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMwChlsfdVunH3w+KVrMIjv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aaafa1cc8e7e4fc19864a1ab19a4772a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bfc5084e08444c485786ffaee1d5c19",
              "IPY_MODEL_50804b8c316442409183bc330f1bd4f7",
              "IPY_MODEL_41395cdf38cf4d209f55c21adfcb99e6"
            ],
            "layout": "IPY_MODEL_81f34221d2b24796a225145455e2e7d0"
          }
        },
        "8bfc5084e08444c485786ffaee1d5c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a79cd62ae64c248a7638a1223947fc",
            "placeholder": "​",
            "style": "IPY_MODEL_a16103ba172b4149b7a8c0f523ef41a6",
            "value": "README.md: "
          }
        },
        "50804b8c316442409183bc330f1bd4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e66f027413fc421da1e60fece25b7eb4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f9b203c03bb4397baacae1069a76d79",
            "value": 1
          }
        },
        "41395cdf38cf4d209f55c21adfcb99e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25158c21696c4d8c8c6723f85409325a",
            "placeholder": "​",
            "style": "IPY_MODEL_54753dd17ccc4dfba045f4320863fb9a",
            "value": " 3.28k/? [00:00&lt;00:00, 372kB/s]"
          }
        },
        "81f34221d2b24796a225145455e2e7d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76a79cd62ae64c248a7638a1223947fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16103ba172b4149b7a8c0f523ef41a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e66f027413fc421da1e60fece25b7eb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "7f9b203c03bb4397baacae1069a76d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "25158c21696c4d8c8c6723f85409325a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54753dd17ccc4dfba045f4320863fb9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a603af7ea9504f66952f861afbf45e78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f94be8cd1b642c7992e1e4dbc7fa4ff",
              "IPY_MODEL_b87330a2a6e941fba0d902d358c6e190",
              "IPY_MODEL_6d2ca2d21f0942449a93f20e4b484ed9"
            ],
            "layout": "IPY_MODEL_ae5129e80aec4a7b9e6e230b9c0f20d8"
          }
        },
        "5f94be8cd1b642c7992e1e4dbc7fa4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb232d3924c04b5a934d99d6d5c0177e",
            "placeholder": "​",
            "style": "IPY_MODEL_d8923918c8fd49da9632ab0763e95903",
            "value": "Resolving data files: 100%"
          }
        },
        "b87330a2a6e941fba0d902d358c6e190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db0b751c0c194b7b9bf2f3320c0b610a",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c95bd1a6513402b96635c696df966ea",
            "value": 33
          }
        },
        "6d2ca2d21f0942449a93f20e4b484ed9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6639c89653974c5c958ae78bda43942d",
            "placeholder": "​",
            "style": "IPY_MODEL_78494f9e47ec4f34870c75081bfc01d9",
            "value": " 33/33 [00:00&lt;00:00,  4.25it/s]"
          }
        },
        "ae5129e80aec4a7b9e6e230b9c0f20d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb232d3924c04b5a934d99d6d5c0177e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8923918c8fd49da9632ab0763e95903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db0b751c0c194b7b9bf2f3320c0b610a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c95bd1a6513402b96635c696df966ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6639c89653974c5c958ae78bda43942d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78494f9e47ec4f34870c75081bfc01d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "793a0dfb2b6845b58126a194d3fd10f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9127ba42c7c84742ad5f011cd0b5e5dd",
              "IPY_MODEL_785cc0c723b04184a83f110e2e3bde56",
              "IPY_MODEL_e18f6ea4a724455d8f5792b659b27c7c"
            ],
            "layout": "IPY_MODEL_79c3f8a07e8d49e38aca0249304f2048"
          }
        },
        "9127ba42c7c84742ad5f011cd0b5e5dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55e4207e86aa42c9878659c01792c0e4",
            "placeholder": "​",
            "style": "IPY_MODEL_6e6dd8396d134c6c9f9894b59d62b77c",
            "value": "Resolving data files: 100%"
          }
        },
        "785cc0c723b04184a83f110e2e3bde56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_270841d734fb478fa4f3dc3efba8c4d4",
            "max": 33,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c56fd224ac5a4c1eb4653f06cacc33c6",
            "value": 33
          }
        },
        "e18f6ea4a724455d8f5792b659b27c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af25c4d0165a4410b3f52b526a893b84",
            "placeholder": "​",
            "style": "IPY_MODEL_4501ad5fadd14d7cb1caf10e51304b85",
            "value": " 33/33 [00:00&lt;00:00, 70.60it/s]"
          }
        },
        "79c3f8a07e8d49e38aca0249304f2048": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55e4207e86aa42c9878659c01792c0e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e6dd8396d134c6c9f9894b59d62b77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "270841d734fb478fa4f3dc3efba8c4d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c56fd224ac5a4c1eb4653f06cacc33c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af25c4d0165a4410b3f52b526a893b84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4501ad5fadd14d7cb1caf10e51304b85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}